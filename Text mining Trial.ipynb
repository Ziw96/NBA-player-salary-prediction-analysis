{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Besides the analysis in the main notebook, we also did an experiment in text mining. Since there are limits in accessing tweets, there are not enough text data, so we only use it as an additional trial here instead of including it in the main line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Twitter API to access the text data about players\n",
    "We use Twitter API to access the text data, and select netizens’ comments on NBA active players and scrape them as texts. We collect approximately 80000 tweets about 540 players, which shows their performance this NBA season, and export them as uniform form for every player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key = \"yujgy2FPd9IOcQsR7Y0co19cI\"\n",
    "consumer_secret = \"OPZdiMIZzA7Jm00Z7PoNIQOfuqdC1T9XMHzk4BDS6tRE8qcLEE\"\n",
    "access_token = \"1183568791772504066-galXwo5tkeGIhcfRo9HkdHfTdIl5od\"\n",
    "access_token_secret = \"Kba56HaU3BGVRDd6Jr7EUjcNzTQ5Ag9dzMUt2Y0B1nNOh\"\n",
    "# you can use these passwords to examine the correctness of our codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(columns = ['Tweets', 'User', 'User_statuses_count', \n",
    "                             'user_followers', 'User_location', 'User_verified',\n",
    "                             'fav_count', 'rt_count', 'tweet_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_players = pd.read_excel('player18.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the name data for players in Season 2018-2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "540"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "players = df_players['Player'] \n",
    "len(players) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dd\n",
    "df1 = pd.DataFrame(columns = ['date','Tweets', 'User', 'User_statuses_count', \n",
    "                             'user_followers', 'User_location', 'User_verified',\n",
    "                             'fav_count', 'rt_count', 'tweet_date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original way of using API to get the data in a single document, the original code manages to return all the data with tweepy.Cursor, while it will report an error 429 as follows because of data getting limit of Twiiter API\n",
    "\n",
    "'''The error is just for showing the traditional low-efficient way and we improve it'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2234\r"
     ]
    },
    {
     "ename": "TweepError",
     "evalue": "Twitter error response: status code = 429",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTweepError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-d167a5ba1ad9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mi_lst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m29\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtweepy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCursor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msearch_term\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msince\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"2019-11-\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muntil\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"2019-11-\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'en'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'\\r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[0mdf1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'date'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtweet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreated_at\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'%Y-%m-%d'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tweepy\\cursor.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tweepy\\cursor.py\u001b[0m in \u001b[0;36mnext\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    193\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_page\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage_index\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_page\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m             \u001b[1;31m# Reached end of current page, get the next page...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 195\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_page\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage_iterator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage_index\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tweepy\\cursor.py\u001b[0m in \u001b[0;36mnext\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mRawParser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__self__'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tweepy\\binder.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    248\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 250\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    251\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tweepy\\binder.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    231\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mRateLimitError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 233\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mTweepError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mapi_code\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mapi_error_code\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m             \u001b[1;31m# Parse the response payload\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTweepError\u001b[0m: Twitter error response: status code = 429"
     ]
    }
   ],
   "source": [
    "write = pd.ExcelWriter('all_pla_cri.xlsx')\n",
    "for player in players2:\n",
    "    df1 = pd.DataFrame(columns = ['date','Tweets', 'User', 'User_statuses_count', \n",
    "                             'user_followers', 'User_location', 'User_verified',\n",
    "                             'fav_count', 'rt_count', 'tweet_date'])\n",
    "    search_term = player\n",
    "    i = 0\n",
    "    i_lst = [0,]\n",
    "    for j in range(21,29): # because of the 7-day constraint of twitter API data, we can only get the data from Nov 21th to Nov 28th on Nov 28th\n",
    "        for tweet in tweepy.Cursor(api.search, q=search_term, count=100, since=\"2019-11-\"+str(j), until=\"2019-11-\"+str(j+1), lang='en').items():\n",
    "            print(i, end='\\r')\n",
    "            df1.loc[i,'date'] = tweet.created_at.strftime('%Y-%m-%d')\n",
    "            df1.loc[i, 'Tweets'] = tweet.text\n",
    "            df1.loc[i, 'User'] = tweet.user.name\n",
    "            df1.loc[i, 'User_statuses_count'] = tweet.user.statuses_count        \n",
    "            df1.loc[i, 'user_followers'] = tweet.user.followers_count\n",
    "            df1.loc[i, 'User_location'] = tweet.user.location\n",
    "            df1.loc[i, 'User_verified'] = tweet.user.verified\n",
    "            df1.loc[i, 'fav_count'] = tweet.favorite_count\n",
    "            df1.loc[i, 'rt_count'] = tweet.retweet_count\n",
    "            df1.loc[i, 'tweet_date'] = tweet.created_at\n",
    "            i+=1\n",
    "            if i == 1500+i_lst[j-21]: #if there are more than 1500 tweets for a player, return only the first 1500 of them.\n",
    "                break\n",
    "        i_lst.append(i)\n",
    "    df1.to_excel(write,sheet_name='{}'.format(str(player)),index=False) # use the names of the player as names of the sheet\n",
    "write.save()    # the original code manages to return all the data with tweepy.Cursor, while it will report an error 429 because of data getting limit of Twiiter API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual implemented way of getting the data 20 players at a time. Run this code will take a few days totally. The final results are in the zip file\n",
    "\n",
    "<b>'''As for the large dataset, Twitter API will take a long time to run'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1664\r"
     ]
    }
   ],
   "source": [
    "for i in range(0,540,20):\n",
    "    players2 = players[i:i+20] # get the data of 20 players at a time\n",
    "    write = pd.ExcelWriter('all_pla_cri{}.xlsx'.format(str(i))) #create a xlsx file for every 20 players in the player list\n",
    "    for player in players2:\n",
    "        df1 = pd.DataFrame(columns = ['date','Tweets', 'User', 'User_statuses_count', \n",
    "                             'user_followers', 'User_location', 'User_verified',\n",
    "                             'fav_count', 'rt_count', 'tweet_date'])\n",
    "        search_term = player\n",
    "        i = 0\n",
    "        i_lst = [0,]\n",
    "        for j in range(21,29):\n",
    "            for tweet in tweepy.Cursor(api.search, q=search_term, count=100, since=\"2019-11-\"+str(j), until=\"2019-11-\"+str(j+1), lang='en').items():\n",
    "                print(i, end='\\r')\n",
    "                df1.loc[i,'date'] = tweet.created_at.strftime('%Y-%m-%d')\n",
    "                df1.loc[i, 'Tweets'] = tweet.text\n",
    "                df1.loc[i, 'User'] = tweet.user.name\n",
    "                df1.loc[i, 'User_statuses_count'] = tweet.user.statuses_count        \n",
    "                df1.loc[i, 'user_followers'] = tweet.user.followers_count\n",
    "                df1.loc[i, 'User_location'] = tweet.user.location\n",
    "                df1.loc[i, 'User_verified'] = tweet.user.verified\n",
    "                df1.loc[i, 'fav_count'] = tweet.favorite_count\n",
    "                df1.loc[i, 'rt_count'] = tweet.retweet_count\n",
    "                df1.loc[i, 'tweet_date'] = tweet.created_at\n",
    "                i+=1\n",
    "                if i == 1500+i_lst[j-21]:\n",
    "                    break\n",
    "            i_lst.append(i)\n",
    "        df1.to_excel(write,sheet_name='{}'.format(str(player)),index=False)\n",
    "    write.save()\n",
    "    time.sleep(600)  # use sleep(600) so that we can run the codes without encountering 429 error\n",
    "    # this is the code we finally used to get the data and stored the data for every 20 players into a file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Form a list in which each element represents the name of an output document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "lst=[i for i in range(0,540,20)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_players = pd.read_excel('player18.xlsx')\n",
    "players = list(df_players['Player'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>'''As for the large dataset, this type transformation process will take a long time to run'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,player in enumerate(players):\n",
    "    for j in range(len(lst)-1):\n",
    "        if i<lst[j+1] and i>=lst[j]:\n",
    "            playerfr = pd.read_excel('all_pla_cri'+str(lst[j])+'.xlsx',sheet_name=player)\n",
    "            f = open(\"{}.txt\".format(player),\"w+\",encoding='utf-8')\n",
    "            for k in range(len(playerfr)):\n",
    "                f.write(playerfr[\"Tweets\"][k])\n",
    "            f.close() # write the data from the sheets of output files into text files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output the tweet text files used for analyzing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "import pandas as pd\n",
    "df_players = pd.read_excel('player18.xlsx')\n",
    "players = list(df_players['Player'])\n",
    "excel_code=\"text_doc/\"\n",
    "tweets_root = excel_code\n",
    "player_lst=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>'''As for the large dataset, this type transformation process will take a long time to run'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,player in enumerate(players):  \n",
    "    for j in range(len(lst)-1):\n",
    "        if i<lst[j+1] and i>=lst[j]:\n",
    "            playerfr = pd.read_excel(excel_code+'all_pla_cri'+str(lst[j])+'.xlsx',sheet_name=player)\n",
    "            f = open(\"{}.txt\".format(player),\"w+\",encoding='utf-8')\n",
    "            for k in range(len(playerfr)):\n",
    "                f.write(playerfr[\"Tweets\"][k])\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use nrc data on sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do data cleaning by the following 2 rules: \n",
    "First, eliminate the players with scarce text (only have a few comments on Twitter ) \n",
    "Second, eliminate duplicate text in each player’s corpus ( same contents below the same player)\n",
    "\n",
    "After cleaning, we build the data arrays used for comparative_emotion_analyzer，divide words into two major categories (Positive & Negative) and 8 different sentiment types (Surprise, Anticipation, Trust, Joy, Disgust, Anger, Sadness and Fear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,player in enumerate(players): \n",
    "    player_file = \"{}.*\".format(player)\n",
    "    player_data = PlaintextCorpusReader(tweets_root,player_file)\n",
    "    try:\n",
    "        player_lst.append([player,player_data.raw()])\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>'''As for the large dataset, this sentiment analysis will take a long time to run'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nrc_data():   # the get_nrc_data/emotion_analyzer/comparative_emotion_analyzer function used in class\n",
    "    nrc = \"NRC-emotion-lexicon-wordlevel-alphabetized-v0.92.txt\"\n",
    "    count=0\n",
    "    emotion_dict=dict()\n",
    "    with open(nrc,'r') as f:\n",
    "        all_lines = list()\n",
    "        for line in f:\n",
    "            if count < 46:\n",
    "                count+=1\n",
    "                continue\n",
    "            line = line.strip().split('\\t')\n",
    "            if int(line[2]) == 1:\n",
    "                if emotion_dict.get(line[0]):\n",
    "                    emotion_dict[line[0]].append(line[1])\n",
    "                else:\n",
    "                    emotion_dict[line[0]] = [line[1]]\n",
    "    return emotion_dict\n",
    "\n",
    "\n",
    "def emotion_analyzer(text,emotion_dict=get_nrc_data()):\n",
    "    #Set up the result dictionary\n",
    "    emotions = {x for y in emotion_dict.values() for x in y}\n",
    "    emotion_count = dict()\n",
    "    for emotion in emotions:\n",
    "        emotion_count[emotion] = 0\n",
    "\n",
    "    #Analyze the text and normalize by total number of words\n",
    "    total_words = len(text.split())\n",
    "    for word in text.split():\n",
    "        if emotion_dict.get(word):\n",
    "            for emotion in emotion_dict.get(word):\n",
    "                emotion_count[emotion] += 1/len(text.split())\n",
    "    return emotion_count\n",
    "\n",
    "def comparative_emotion_analyzer(text_tuples,object_name,print_output=False):\n",
    "    if print_output:\n",
    "        print(\"%-20s %1s\\t%1s %1s %1s %1s   %1s %1s %1s %1s\"%(object_name,\n",
    "                                                              \"negative\",\"positive\",\"surprise\",\n",
    "                                                              \"anticipation\",\"trust\",\"joy\",\"disgust\",\"anger\",\"sadness\",\"fear\"))\n",
    "                                                              \n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(columns=[object_name,'Negative',\n",
    "                           'Positive','Surprise','Anticipation',\n",
    "                           'Trust',\"Joy\",\"Disgust\",\"Anger\",\"Sadness\",\"Fear\"])\n",
    "    df.set_index(object_name,inplace=True)\n",
    "    \n",
    "    output = df\n",
    "    for text_tuple in text_tuples:\n",
    "        text = text_tuple[1] \n",
    "        result = emotion_analyzer(text)\n",
    "        if print_output:\n",
    "            print(\"%-20s %1.2f\\t%1.2f\\t%1.2f\\t%1.2f\\t%1.2f\\t%1.2f\\t%1.2f\\t%1.2f\\t%1.2f\"%(\n",
    "                text_tuple[1][0:20],result['negative'],result['positive'],result['surprise'],\n",
    "                  result['anticipation'],result['trust'],result['joy'],result['disgust'],result['anger'],result['sadness'],result['fear']))\n",
    "        df.loc[text_tuple[0]] = [result['negative'],result['positive'],result['surprise'],\n",
    "                  result['anticipation'],result['trust'],result['joy'],result['disgust'],result['anger'],result['sadness'],result['fear']]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dff=comparative_emotion_analyzer(player_lst,object_name='player',print_output=False) #run the built function to get "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff.to_excel('emo_player-all.xlsx') #output the analyzing result to excel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Machine Learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.read_csv('Final data.csv',index_col='Unnamed: 0')\n",
    "df2=pd.read_excel('emo_player-all.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df1[1:]\n",
    "df1=df1[df1['Year']==2018]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2018], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['Year'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.merge(df1,df2,left_on=[\"Name\"],right_on=[\"player\"],how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns\n",
    "df=df[['Name', 'tenure', 'AGE',\n",
    "       'GP', 'W', 'MIN', 'PTS', 'FGM', 'FGA', 'FG%', '3PA', '3P%',\n",
    "       'FTM', 'FTA', 'FT%', 'OREB', 'DREB', 'REB', 'AST', 'TOV', 'STL', 'BLK',\n",
    "       'PF', 'FP', 'DD2', '+/-', 'Height', 'Weight',\n",
    "       'Salary_Weight', 'Salary_Weight_l',\n",
    "       'Negative', 'Positive', 'Positive/Negative', 'Surprise', 'Anticipation',\n",
    "       'Trust', 'Joy', 'Disgust', 'Anger', 'Sadness', 'Fear']]\n",
    "x_train=df[['tenure', 'AGE',\n",
    "       'GP', 'W', 'MIN', 'PTS', 'FGM', 'FGA', 'FG%', '3PA', '3P%',\n",
    "       'FTM', 'FTA', 'FT%', 'OREB', 'DREB', 'REB', 'AST', 'TOV', 'STL', 'BLK',\n",
    "       'PF', 'FP', 'DD2', '+/-', 'Height', 'Weight',\n",
    "        'Salary_Weight_l',\n",
    "       'Negative', 'Positive', 'Positive/Negative', 'Surprise', 'Anticipation',\n",
    "       'Trust', 'Joy', 'Disgust', 'Anger', 'Sadness', 'Fear']]\n",
    "y_train=df['Salary_Weight']\n",
    "x_train2=df[['tenure', 'AGE',\n",
    "       'GP', 'W', 'MIN', 'PTS', 'FGM', 'FGA', 'FG%', '3PA', '3P%',\n",
    "       'FTM', 'FTA', 'FT%', 'OREB', 'DREB', 'REB', 'AST', 'TOV', 'STL', 'BLK',\n",
    "       'PF', 'FP', 'DD2', '+/-', 'Height', 'Weight',\n",
    "        'Salary_Weight_l']]      # the names of the columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using our original best model Random Forest under Auto-Regression Assumption, we decided to apply bagging method under Auto-Regression Assumption to the the new data set with sentiment features due to a limit in the data size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.06561011360407598,\n",
       " {'max_features': 20,\n",
       "  'max_samples': 0.5,\n",
       "  'n_estimators': 500,\n",
       "  'random_state': 888})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "import math\n",
    "\n",
    "parameters = {\n",
    "     'n_estimators':[500], #the number of trees\n",
    "     'max_samples':(0.5, 0.7, 0.8),\n",
    "     'max_features':[20],\n",
    "     'random_state':[888]\n",
    "     #'oob_score':np.ravel(y_test)\n",
    "     \n",
    "}\n",
    "\n",
    "model = GridSearchCV(BaggingRegressor(),parameters,cv=10,iid=False,scoring='neg_mean_squared_error')\n",
    "model.fit(x_train, y_train)\n",
    "math.sqrt(-model.best_score_), model.best_params_  # best model parameters are n_estimators: 500, and max_features: 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.06496773952486219,\n",
       " {'max_features': 20,\n",
       "  'max_samples': 0.7,\n",
       "  'n_estimators': 500,\n",
       "  'random_state': 888})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters2 = {\n",
    "     'n_estimators':[500], #the number of trees\n",
    "     'max_samples':(0.5, 0.7, 0.8),\n",
    "     'max_features':[20],\n",
    "     'random_state':[888]\n",
    "     #'oob_score':np.ravel(y_test)\n",
    "     \n",
    "}\n",
    "\n",
    "model = GridSearchCV(BaggingRegressor(),parameters2,cv=10,iid=False,scoring='neg_mean_squared_error')\n",
    "model.fit(x_train2, y_train)\n",
    "math.sqrt(-model.best_score_), model.best_params_ # this model includes sentiment attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
